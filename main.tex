\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage[at]{easylist}

\newcommand{\ham}{\mathcal{H}}

\title{A Rigorous Proof of the Symmetry-Based Molecular Orbital Method \\ \large \#SoME4 Submission}
\date{August 2025}

\begin{document}

\maketitle

\section{Preface}
Symmetry methods are taught universally in undergraduate inorganic classrooms, and have been for many years. Surprisingly, however, there are few resources available which cover the subject in deeper depth from a mathematical perspective, and of those, none are both accessible to undergraduates and detailed enough to provide a complete understanding. Some advanced texts exist from a mathematical or physics perspective, but the vast majority are heavily tailored towards advanced application (in the case of phyics) or highly abstract generalization (in the case of math), rendering them pretty much useless to nonexperts in those fields. A small number of advanced chemistry texts cover the subject - namely, Robert Carter's \textit{Molecular Symmetry and Group Theory} and F. Albert Cotton's \textit{Chemical Applications of Group Theory} each go into some detail - but both either fall short of a full-depth explanation (in the case of carter) or provide a significant amount of rigor, but don't elaborate enough about the representation theory before jumping right into character theory, which in Cotton's book left me particularly unsatisfied. Additionally, neither do a very thorough job of motivating the method physically, arguably the most important part of a grounded and satisfying explanation. 

I personally think a thorough explanation and \textit{demonstration} of the representation theory of molecules is a very useful exercise, even if actually applying the method without character theory becomes rather impractical to do by hand. Not only does it provide a strong foundation from which character theory builds upon, but it directly leads to some very interesting intuitive insights. So, my goal with this writeup is threefold: 1) lay out thoroughly and rigorously the representation theory of symmetry groups of molecules; 2) deeply motivate why understanding group and representation theory helps us approximate molecular orbitals efficiently; and 3) apply the method to several molecules, with detailed explanations and example matrices. These are the three aspects I was looking for when I first set out to understand this content, and I hope this could be the resource I was looking for.

However, this is probably not the resource that most chemistry students are looking for. While many mathematical insights can come from a rigorous approach with abstract tools, there's a significant amount of visual intuition that also helps a great deal, and that's something I can't provide in text. However, I am also working on a youtube video made with Manim aimed at an audience with a less thorough mathematical background. I had originally intended that to be my SoME4 submission, but I was hit with the paradox of teaching: It's harder and takes longer to write a shorter, simpler explanation than a longer, more rigorous one ("If I had had more time, I would have written a shorter letter" -CGP Grey). My goal is for that video to be as rigorous and satisfying as this writeup, albeit with some of the more involved (and less interesting) proofs to this document, and it will focus much more on the visual and chemical intuition. That's not to say that I won't try to make this document intuitive - it's just that I can't guarantee groundbreaking revelations to anyone who doesn't already have a very strong intuition via text alone about linear algebra and groups. (If you want to take a stab at it anyway, then hell yeah! I do really recommend 3blue1brown's essence of linear algebra series (\url{https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE\_ab}) as an introduction to the visual intuition of linear algebra, which has been indispensable for me in my research and in writing this video).

Okay, last thing before I shut up and start talking: I won't be covering character theory in detail here. As I mentioned before, using character tables to find multiplicity of irreducible representations is a very useful trick, and when combined with projection to extract basis vectors, it can fully skip the need to do decomposition manually; however, all explanations of it (as they exist online, which is nearly as rare) are necessarily rooted in the theory presented here, so to understand them, it's critical to understand representation theory and invariant subspaces. If all goes to plan, I do intend to make a follow up video + article about character theory and the projection operator method. For now, I explain at the very end exactly what character theory \textit{does} in the context of representation theory, and briefly outline the projection operator method (just to get it written down) and its role in completing the shortcut.


\section{Introduction: Symmetry, Group Theory, and Molecules}
The fundamental problem we will be tackling today is very simple to describe: You have a molecule. You know its atoms and structure, but you want to know its bonds. More specifically, you want to know the energy and shape of the various orbitals its electrons can occupy. In other words, you want it's \textbf{molecular orbitals}: the wavefunction describing the position of an electron associated with each electronic state.

You already know how to do this for atoms - either because they had you derive it in a physics or physical chemistry class, or simply because you've seen them every time you enter a chemistry class: s orbitals, p orbitals, d orbitals, etc, each parameterized by shell and angular momentum. Although atomic number affects the magnitude of the nucleus' pull on its resident electrons, the shapes are all in the family of hydrogen-like orbitals: the first electronic wavefunctions to be solved analytically in closed form in the 1920s\footnote{Really, they were solved by Laplace in 1782 as the spherical harmonics, and physicists only realized that they \textit{applied} to the hydrogen wavefunctions when they \textbf{invented wavefunctions} in the '20s.}. 

In fact, they were the only wavefunctions to be solved in closed form. They still are. It turns out having multiple nuclei tugging on electron clouds is too much for the tools of modern differential calculus to handle; in fact, the hydrogen orbitals can't even be symbolically solved when there is more than one electron around the molecule for the same reason. 

This leaves us in a bit of a predicament with our molecular orbital task. If we can't solve the equations that are supposed to produce them, how can we ever be expected to find them?

The answer is twofold. We will, at the end of the day, be using an approximation which lets us turn the problem into something more tractable. More interestingly, however, we will be searching for the orbitals by determining properties we know they must have, and then guiding our search accordingly. And for that, we will be using \textbf{molecular symmetry}.


\section{Molecular Symmetry}
For the sake of time, I will be assuming you know the basics of molecular geometry and molecular symmetry. There are many clear and concise resources on Google and YouTube that walk one through the various symmetry operations and assigning point-groups to molecules. I will, however, talk briefly about \textit{why} we care about symmetry.

Each molecule has a particular arrangement of atoms which defines its geometry. Since we are only going to be solving for single-electron orbitals (because Fock knows that's hard enough\footnote{Hart[ree] enough} on its own), we can assume that the electric force/potential field which will determine the shapes of our orbitals is influenced solely by the positions (and nuclear charge) of each atomic nucleus. Thus, for any given molecule, there will very often be some number of rearrangements of the atoms which will leave the electric potential - and thus the molecular orbitals - entirely unchanged. These rearrangements are known as the \textbf{symmetries} of our molecule, and, taken as a whole, the set of a molecule's symmetries form a \textbf{mathematical group} (whose group action is the act of composing symmetry actions, e.g. performing one action, then the other).

While it might not seem like a big deal, these actions rigorously encode a set of equalities we can use to characterize the potential field and thus the orbitals generated by it. Since we generally only consider the symmetry actions which can be described as transformations of space - reflections, rotations, and inversions - we can consider the effect of a symmetry action on any object in the molecule's 3d space. Notably, if we consider the \textbf{chemical environment} of a particular wavefunction - that is, the potential around it and the forces thus imparted that would change the shape of the wavefunction over time - we know it must be the \textbf{same} as the chemical environment of any wavefunction symmetrical to the first, such that applying a symmetry to the first can produce the second. 

This means that if two wavefunctions are symmetrical to each other, they must exist in the same chemical environment as each other, and therefore, they will both \textit{evolve} symmetrically to each other over time. Since the \textbf{stable molecular orbitals} of a molecule are those which \textbf{don't change over time}\footnote{except for a global phase offset - keep reading!}, that means if two wavefunctions are symmetrical to each other, and one of them is a molecular orbital, the other is a molecular orbital as well. And remember, "symmetrical to each other" means there exists a symmetry action which brings one to the other - so if you have a molecular orbital (MO), then all of the wavefunctions you can get by applying the symmetries of the molecule to that MO will also be MOs. Additionally, since the energy of an orbital is a property of its shape in the potential well, all of those equivalent MOs will have the same definite energy as the first. 

What this tells us is that space of all possible wavefunctions can be grouped into symmetric sets, and that our molecular orbitals will always be grouped with other identical MOs. What the remainder of this writeup will document is how we can classify these symmetric sets - and the linear spaces they form - both by their size and by the action of the group upon them. I will then show that by grouping together these symmetric spaces by their classification, and by finding a basis for these groupings using the atomic orbitals of our atoms, we can greatly simplify the process of determining our molecular orbitals.



\section{Orbits and Invariant Subspaces}
In the field of group theory, these "symmetric sets" are known as \textbf{orbits}. Specifically, if a group $G$ describes a set of \textit{actions} on a set, in this case the set of wavefunctions, then the orbit under $G$ (written $\text{orbit}_G$) some element $\Psi$ of that set is defined as $\{g * \Psi | g \in G\}$ - that is, the result of applying $g$ to $\Psi$ for each action $g$ in $G$.

It's important to note that, although every element will have its own orbit, not all orbits are equivalent. For example, some orbits will be different sizes; some objects might be totally symmetric with respect to the group, so that every symmetry action maps that element to itself. In that case, the orbit would only be a single element! On the flipside, some elements might be totally asymmetric, so that every symmetry action will result in a different element. Other orbits might be the same size as each other but be the result of the group acting differently on the seed element - for example, one seed element might be totally symmetric under reflection, but not rotation, while the other seed element is totally symmetric under rotation, but not reflection. This inherent structure to how the group acts on an orbit is what we're interested in describing, for reasons that will be made clear in the next section.

However, describing the action of the group on each orbital is a very complicated task, and for a general group, doesn't actually help us that much. But this is no general group! Remember, we defined our symmetry group to be composed solely of \textbf{transformations of space}. This means that the action of our group on the space of \textit{wavefunctions} can be described linearly.\footnotemark
\footnotetext{I hope this is obvious visually or intuitively - if you move a sum of functions in space, then the result should be the same as if you moved the individual functions then added them together. This generalizes to any transformation of space. Since I want this document to be rigorous, a formal proof (apologies, it's quite contrived):
Consider the space of functions $F$, where each element $f$ in $F$ is of the form $f: A \to V$. $X$ can be any space, but $V$ is a vector space (over a field $K$), which means $F$ is also a vector space, with $f_1 + f_2$ defined such that $(f_1 + f_2)(a) = f_1(a) + f_2(a) \quad \forall a \in A$. Suppose there is some transformation of the input to f $T: A \to A$. We want to describe transforming f by way of transforming the space it acts on by $T$. To do so, let us define $R: F \to F$, where $R(f) := f\circ T$; in other words, $R(f)(a) := f(T(a))\ \  \forall a \in A$. The claim is that $R$ is linear on $F$. Proof: Consider $f$ = $xf_1 + yf_2$ for $x$, $y$ scalars in $K$. To prove $R$ is linear, I claim $R(f) = xR(f_1) + yR(f_2)$. By expansion: $R(f) = R(xf_1 + yf_2)$. By definition of $R$, this equals $(xf_1 + yf_2)\circ T$, which equals $x(f_1\circ T) + y(f_2\circ T)$ by the linearity of functions - that is, $(xf_1 + yf_2)(T(a)) = xf_1(T(a)) + yf_2(T(a))\ \ \forall a \in A$. Then, applying the definition of $R$, $f_1(T(a)) = R(f_1)$ and $f_2(T(a)) = R(f_2)$, so $R(f) = xR(f_1) + yR(f_2) \square$. $R$ here is also sometimes known as the \textbf{pullback operator} over $T$, such that $R(f)$ is the \textbf{pullback of f by T}. There's a connection to the transpose, but I don't really understand it and this footnote is already wayyyyy too long. }

This opens up the door to many possibilities. First, it means that we can describe the group action with matrices, which computationally opens the door to a huge number of possibilities that linear algebra provides. Secondly, it allows us to relate the various orbits-of-wavefunctions to each other by way of these matrices. We will address both of these using \textbf{invariant subspaces}. I'm assuming you have a decent background in linear algebra, but as a brief reminder, a subspace of some larger vector space $V$ is a subset of $V$ in which addition and scalar multiplication are closed. You can think of it as some n-dimensional "slice" of $V$ - for example, an (infinite) line, an (infinite) plane, and the point at the origin are all subspaces of $V$ - 1, 2, and 0 dimensional subspaces, respectively. For a subspace to be \textbf{invariant} under a linear action - or in our case, a set of them - means that every vector in that space gets mapped to another vector in that space under the action (or under all actions in the set). In other words, each action reorganizes the vectors in the subspace - rotating, reflecting, or otherwise transforming them - but does not move any vectors out of (nor, because group actions are always invertible, into) the space.

Invariant subspaces are very useful when trying to understand a (set of) linear action(s), because any linear action can be represented on a (finite) invariant subspace as matrix. Specifically, given a basis ${b_1,b_2,...}$ of vectors in that subspace, each element of the matrix representation of a linear action $g$ can be defined as $a_ij = b_i\cdot g*b_j$. It's important that the subspace be invariant so that $g*b_j$ can always be expressed as a linear combination of your basis! For a given invariant subspace (and basis), the set of matrices corresponding to each linear action of the group is known as a \textbf{representation} of the group on that space\footnote{More rigorously, the representation is a \textbf{map} from each element in the group to the corresponding matrix representation}. Although the actual entries in each matrix are dependent on the particular basis, we often like to talk generally about the group's action on the space as a whole independent of the basis. To that end, we usually call two representations of a group the same if one can be transformed into the other by a \textbf{change-of-basis} - algebraically, if for each matrix $A_i$ from the first representation, the corresponding matrix in the second can be described as $MA_iM^{-1}$, using the same change-of-basis matrix $M$ for each element in the representation. Thus, up to this notion of equivalence, every invariant subspace can be classified by the representation of the group on that subspace, which describes how that subspace is linearly transformed by the different actions of the group.

Critically, \textbf{the span of the orbit of a vector is always an invariant subspace}. Recall that the span of a set of vectors is the set of all possible linear combinations of those vectors, and is always a valid subspace. Lets say the orbit of a vector $v_1$ is $\{v_1, v_2, ...\}$, and will call the subspace $V = \text{span}\{v_1, v_2, ...\}$. Then each element in the subspace can be described as $v = c_1v_1  + c_2v_2 + ...$. The subspace $V$ is invariant under $G$ iff, for every vector $v$ in $V$, $g*v \in V$ for every $g \in G$. $g*v = g*(c_1v_1 + c_2v_2 + ...) = c_1g(v_1) + c_2g(v_2) + ...$ by linearity of $g$. Note that the orbit itself must be invariant under $G$, as if some element $v_i \in \text{orbit}_G(v) = g*v$, then $h*v_i = h*(g*v) = (h*g)*v$; since $h*g \in G$, $(h*g)*v \in \text{orbit}_G(v)$. Thus, $g*v_i \in V$, meaning $g*v$ is some linear combination of vectors in $V$ - which by the definition of a subspace, means $g*v \in V$. Thus, the span of an orbit of a vector forms an invariant subspace. 
%For brevity, I will refer to these invariant subspaces as the \textbf{orbit-space} of v: $\text{orbitspace}(v) := \text{span}\left(\text{orbit}(v)\right)$. 

With these two tools, we can return to our original goal of classifying the symmetric sets of our wavefunctions, by extending that to classifying the symmetric \textit{spaces} of wavefunctions and using those classifications to compute the value of our molecular orbitals.

\section{Invariant Subspaces of MOs}
Earlier I mentioned that we know that molecular orbitals come in symmetric sets because the chemical potential of the molecule is symmetrical. While this is true, it pays to be a little rigorous, so let's look a little deeper at the physics. At the core of wavefunction evolution is a linear operator known as the \textbf{Hamiltonian}, $\ham$. When I say at the core, I'm being quite literal: it is the fundamental component of the Schr\"odinger equation, which describes how any wavefunction $\Psi$ evolves over time.
\begin{figure}[H]
    \centering
    \begin{equation}
        \frac{\partial{\Psi}}{\partial{t}} = -\frac{i}{\hbar}\ham\Psi
    \end{equation}
    \caption{The Schr{\"o}dinger Equation}
    \label{fig:schrodinger}
\end{figure}

When we talk about atomic or molecular orbitals (of an atom or molecule respectively), what we mean are the solutions to this equation which describe the quantum state of an electron. Specifically, we want the \textbf{time-independent} solutions to this equation, meaning that $\Psi$ doesn't \textbf{observably} change over time. "Observably" is a loaded word there which has implications beyond the scope of this document, but essentially the requirement is that the probability density function describing an electron's state doesn't change. The probability density is the magnitude of the wavefunction squared; since a wavefunction is complex, this means that the wavefunction is allowed to change in \textbf{phase} over time, but not in magnitude. Specifically (and this is where the nuances of observability come in), to be time-independent, the wavefunction is allowed to globally change phase over time - that is, its value at every point must change phase at the same rate - but there must be no changes in magnitude or global phase.

That's a very complicated requirement, but luckily it has a simple(r) solution: $\frac{\partial{\Psi}}{\partial{t}}$ must be a 90\textdegree or -90\textdegree phase-offset scalar multiple of $\Psi$. To realize why, imagine $\Psi$ evaluated a single point $x$, somewhere in the complex plane. If we want to add some value which will infinitesimally rotate $\Psi(x)$ around the origin, the complex number we add must be perpendicular to $\Psi$ such as to not change its distance to the origin. So long as our $d\Psi$ always remains perpendicular as $\Psi$ changes, it will travel in a smooth circle. Since the effect of this rotation depends on the radius to the origin, it's important that the magnitude of our added vector scales with the magnitude of $\Psi$ at every point, so that all of $\Psi$ is rotated at the same rate.

Hopefully that description was illustrative, but if not, fear not: we are at its end. We know that $\frac{\partial{\Psi}}{\partial{t}}$ must equal $\Psi$, rotated 90\textdegree and potentially multiplied by some real number; since $\hbar$ is just a real physical constant, and multiplying by $i$ rotates complex numbers by 90\textdegree in phase, we know that $\ham\Psi =\lambda \Psi$ for some real number $\lambda$. This equation is often known as the \textbf{time-independent Schr\"odinger equation}, as it can be used to calculate all of the time-dependent solutions to the Schr\"odinger equation. 

Those familiar with linear algebra will recognize this as an \textbf{eigenvector} equation. An eigenvector of a linear operator $A$ is any vector $x$ such that $Ax = \lambda x$ for some scalar $\lambda$; the value of $\lambda$ corresponding to a certain eigenvector $x$ is known as the corresponding \textbf{engenvalue}. It turns out the eigenvalues of the Schr\"odinger equation - the eigenvalues of the Hamiltonian operator - are also very important; they represent the corresponding \textbf{energy} of an orbital. That's why in most physics texts, you'll see the above equation written $\ham\Psi = E\Psi$.

Thus, by finding solutions to the time-independent Schr\"odinger equation, we compute both the orbital \textit{and} its energy. How cool is that! However, though eigenvector-eigenvalue problems are arguably the most fundamental class of computational problems in linear algebra, solving them numericalls is often extremely computationally costly. In this case, especially, it's quite expensive, as evaluating $\ham\Psi$ a single time requires a complex 3d integral of the wavefunction \textit{and} the potential field over all of space. It should be clear doing any of that by hand is completely out of the question. The ultimate goal of the symmetry approach, therefore is to \textbf{make this process as easy as possible} - in some cases, making the act of finding solutions not only possible to do by hand, but intuitive.

To do so, we return to symmetry. When we say that the \textbf{evolution} of an orbital is the same as its symmetric counterparts, we are precisely talking about the Hamiltonian: That if one were to move an orbital to a symmetric location, evolve it, and then move it back, the the result would be the same as if you just evolved it in place. Because the Hamiltonian rigorously describes this evolution, we say that for any action $g$ in $G$, $g\circ \ham\circ g^{-1} = \ham$, or just $g\circ\ham = \ham\circ g$. In other words, for any wavefunction $\Psi$, $\ham(g*\Psi) = g*(\ham\Psi)$. 

This has a very interesting property for our eigenvectors in particular - since $\ham\Psi = E\Psi$, $\ham(g*\Psi)=g*(\ham\Psi)=g*(E\Psi)$. Since the action of $g$ is linear, $g*(E\Psi) = E(g*\Psi)$, so $\ham(g*\Psi) = E(g*\Psi)$ - so if $\Psi$ is an eigenvector with eigenvalue $E$, then so is $g*\Psi$! This precisely matches our intuition from before, but it gives us an extra tool, because the set of eigenvectors with the same eigenvalue (which are known as \textit{degenerate} eigenvectors) have a very important property: they form a complete subspace. Specifically, if we have eigenvectors $v_1$ and $v_2$ of an operator $A$, where $Av_1 = \lambda v_1$ and $Av_2 = \lambda v_2$, then for any linear combination $c_1v_1 + c_2v_2$, $A(c_1v_1 + c_2v_2) = c_1A(v_1) + c_2A(v_2) = c_1\lambda v_1 + c_2\lambda v_2 = \lambda(c_1v_1 + c_2v_2)$. In other words, any linear combination of eigenvectors of the same eigenvalue is also a degenerate eigenvector with that eigenvalue.

What this means is the span of the orbit of our eigenvectors - which, remember, is an invariant subspace under $G$ - is an invariant subspace \textbf{entirely composed of degenerate eigenvectors}. This helps us in two ways: 1) we can classify it by the representation of $G$ upon it, and 2) we no longer need to worry about finding a specific set of symmetric molecular orbitals - all we need to do is find a subspace of them, and we can pick any set we like from within it.

Of course, that's still a daunting task. But now that we know that we're looking for invariant subspaces, there's a final trick that will tie everything together.

\section{Representation Spaces and Irreducible Representations}
The fundamental trick that we're going to use to tie everything together is quite simple: If two invariant subspaces induce the same representation of $G$ - where, you'll remember, we define "same" to mean that the matrices are similar by change-of-bases - then together, they form the basis for a larger space in which there are an infinite number of invariant subspaces with that representation. Specifically, say we have two invariant subspaces $U$ and $V$ have representations $A$ and $B$ respectively, and for each $A_g$ and $B_g$ corresponding to the group element $G$, $B_g = MA_gM^-1$ for some change-of-basis matrix $M$. For every vector $u$ in $U$ and $v$ in $V$, and for every vector $w = a_1u + a_2v$ with scalar $a_1$ and $a_2$, then the span of the orbit of $w$ is an invariant subspace which induces a representation of $G$ that is also similar to $A$ and $B$. Proof: Name $W$ as the span of the orbit of $w$. We already know $W$ is invariant under $G$ from before. Take some arbitrary vector $x$ in $w$; I claim we can describe the action of each $g$ on $w$ by way of a matrix $C$ which is similar to both $A$ and $B$. Since we know $W$ is spanned by the orbit of $w$, any $z$ in $W$ can be written as $c_1(g_1*w) + c_2(g_2*w) + ...$ for scalar $c_i$ and over all $g_i$ in $G$. Since $w = u + v$, this further expands to $c_1(g_1*(u + v)) + c_2(g_2*(u+v)) + ... = c_1(g_1*u) + c_1(g_1*v) + c_2(g_2*u) + c_2(g_2*v) + ... = (c_1(g_1*u) + c_2(g_2*u) + ...) + (c_1(g_1*v) + c_2(g_2*v) + ...)$. Since both $U$ and $V$ are invariant under $G$, $g_i*u \in U$ and $g_i*v \in V$ for all $g_i$; thus, \textit{every} vector in $W$ can be expressed as a sum of sum of some vector $x$ in $U$ and $y$ in $V$. In other words, $z = x + y$. Moreover, since the derivation involved only equality, this decomposition is unique. Then, for each group element $g$: $g*z = g*(x + y) = g*x + g*y$. Since $U$ and $V$ have representations $A$ and $B$ respectively, $g*z = A*x + B*y$. To construct our matrix $C$, we'll first define the change of basis. We will use the \textbf{projections} $P_{W\to U}$ and $P_{W\to V}$ from $W$ to $U$ and $V$ respectively: since each $z \in W$ has a unique decomposition $x + y$ into $x\in U$ and $y \in V$, define $P_{W\to U}(z) = x$ and $P_{W\to V}(z) = y$. Thus $g*z = A_gP_{W \to U}*z + B_gP_{W \to V}*z$, meaning $C$ = $AP_{W \to U} + BP_{W \to V}$. Without loss of generality, I'll now prove that $C$ is similar to $A$. Goal: show there exists a change-of-basis (invertible) matrix $M$ such that $C$ = $MAM^{-1}$. First, we there exists some change-of-basis matrix (call it $K$) such that $B = KAK^{-1}$. Thus $C = AP_{W \to U} + KAK^{-1}P_{W \to V}$. Without loss of generality, I will then prove that $C$ is similar to $A$, e.g. $C = MAM^{-1}$ for some invertible matrix $M$. Claim: $C = P_{W \to U}^{-1}AP_{W \to U}$. First, again since $z = x + y$ is unique, we know that $P_{W \to U}^{-1}$ exists; specifically, we can define it to be $P_{U \to W}$, where for $x = c_1(g_1*u) + c_2(g_2*u) + ..$, $P_{U \to W}(x) = z = c_1(g_1*w) + c_2(g_2*w) + ...$. To prove the actual equality, we need to show that for all $z$ in $W$, $P_{U \to W}AP_{W \to U}*z = C*z$. For $z = x + y$, $C*x = A*x + B*y$; $P_{U \to W}AP_{W \to U}*z = P_{U \to W}A*x$. \textbf{TODO: FINISH!!!! I'll need to describe the action of A on x's $c_i$s, then prove that the action of B on y's $c_i$s is the same.}

While the above proof is... very complicated, the meaning is straightforward: If you know the representation of a space, you know that it must be a subspace of the span of all invariant subspaces with that same representation. That is - if you are looking for a subspace with representation $R$, then if you can find a complete set of linearly independent subspaces all with representation $R$, then the subspace you're looking for must have a basis made of linear combinations of vectors from each representation. \textit{This} is why we care so much about invariant subspaces and their representations. If we group all like representations together, then given a representation, we can immediately how to find all the subspaces with that representation. 

This \textit{almost} gives us a formula for finding our molecular orbitals - except that we don't know their representations. There is, however, one final saving grace: We don't have to, because we know they must be \textit{irreducible representations}.

One aspect I didn't mention earlier is the concept of the \textbf{decomposition}, otherwise known as \textbf{reduction}\footnote{the two are technically different, but since we're dealing with finite groups, they are synonymous} of a space into invariant subspaces. By decomposing, I mean finding a collection of invariant subspace, which, together, reconstitute the whole space; for example, the xy plane and the z axis together constitute euclidean 3d space, as any vector in 3d space can be described as a linear combination of a vector from the xy plane and a vector parallel to the z axis. This joining of subspaces is known as the direct sum; while I won't go into detail here, it may be useful to look up the details if you aren't familiar. What's most important is that the subspaces be jointly linearly independent; that is, no vector in one can be written as a linear combination of vectors from any other. This allows us to say that the dimensionality of the whole space is the sum of the dimensions of each individual subspace, and we can find a basis of the whole space by joining bases from each individual space.

An invariant subspace is therefore \textbf{decomposable}, or \textbf{reducible}, if it itself contains linearly-independent invariant subspaces (besides itself and the trivial space {0}) which constitue the space (via the direct sum). A reducible invariant subspace can then be broken down into strictly smaller subspaces. On the flipside, an invariant subspace \textbf{indecomposable}, or \textbf{irreducible}, if it cannot be broken down in this way. All one dimensional invariant spaces are irreducible, as it has no subspaces other than itself and the trivial space; however, there exist higher dimensional ones. We can similarly name the \textit{representations} of these spaces as reducible or irreducible, as indeed, the information of whether a space has invariant subspaces is precisely dependent on how the representation upon it transforms its vectors.

Irreducible representations are fantastically useful. For one thing, finite groups have \textbf{finitely many of them} (as usual, under similarity by change-of-basis). That means, no matter how large your space is, the irreducible representations will only ever be one of the finitely many that correspond to the group. Additionally, a very famous theorem - Matchke's theorem, for which there are \textit{many} proofs online, so I won't give one here - states that any invariant subspace can be decomposed into a complete set of irreducible representations of a finite group, and, moreover, that the \textit{representations} in that decomposition is unique; that is, no matter what subspaces you decompose the larger space into, the number of each irreducible representation in the final decomposition will be the same. This should hopefully make sense, considering the above proof; if one decomposition produces two irreducible subspaces with the same representation, then by taking any two linear independent combinations of vectors from each subspace, you'll get another pair of irreducible subspaces (so long as the two vectors don't produce the same subspace) with the same representation. Thus, multiple copies of an irreducible representation creates a degree of freedom in how you select your subspaces, but as the dimension of that space of degenerate representations is fixed, the number of linearly independent irreducible subspaces you can extract will always be the same.

So: How do we know our eigenvectors will always belong to irreducible representations? Because they form invariant subspaces! Though I should be more specific. I don't mean that \textit{every} eigenvector of the Hamiltonian will necessarily belong to an invariant subspace. While \textit{many chemistry and physics texts claim that every set of eigenvectors with the same eigenvalue is irreducible, \textbf{it is not, in general, true}}\footnote{I won't prove it here - see \url{https://math.stackexchange.com/questions/5090741/the-sum-of-two-vectors-belonging-to-different-irreducible-representations-is-not/5090998} for a proof - but if a degenerate space of eigenvectors contains two irreducible subspaces with different representations, then any linear combination of vectors from each subspace (which must be an eigenvector) will have an orbit that spans the space of both ireps, and therefore not be irreducible.}\footnote{It is this specific caveat which drove me to make this document - I knew I could never reasonably fit it into the video without extensive tangents, and its inclusion was breaking the flow. However, I \textit{needed} this to be corrected, \textit{somewhere}, on the internet, so here it is, in a footnote on page 11 of an extremely dense text. Hooray...}. However, it \textit{is} true that every space of degenerate eigenvectors is \textit{invariant}, which means that it can always be decomposed into a set of irreducible subspaces - which must themselves \textit{also} be spaces of degenerate eigenvectors.\footnote{Also, I will say that I have not once run into a molecule in which two differently-symmetric eigenfunctions have the same energy. The circumstance I safeguard against here by mentioning this caveat is essentially a cosmic coincidence - that two completely unrelated orbitals might, by algebraic happenstance, have the same energy, and thus be degenerate. Usually, the chemist's answer is to pretend that never happens, whereas the physicist's answer is to instead say "there are no accidents" and chalk it up to some fundamental symmetry that our group doesn't properly capture, and that therefore we should actually be using this other better group in which the spaces are more distinct.} This allows us to effectively sidestep this issue, as our only goal is to find a complete basis of eigenvectors; thus, if we can find a basis for each \textit{irreducible} space of degenerate eigenvectors, we will have produced a basis for all spaces of eigenvectors (and indeed, all space). So, for all intents and purposes, we only need to search the irreducible subspaces to find the eigenvectors.

\textbf{Thus, our problem reaches its final form: Determine which irreducible subspaces house the eigenvectors of the Hamiltonian.} The process for this is relatively straightforward: Find a complete decomposition of our space into irreducible representations, combine the subspaces with the same representation via the direct product, and then find a basis of eigenvectors in each smaller space. Since we know we can find a complete basis of eigenvectors in our irreducible spaces, and since we know that for each irreducible representation, every subspace inducing that representation of $G$ can be found as a subspace of the direct product of enough spaces with that representation, we know that each collection of irreducible spaces will contain all of the degenerate spaces of eigenvectors belonging to that representation. 


\section{Wrapping Up}
Actually applying this to molecules is another matter. Despite what it might seem from the density and complexity of this text, however, the process is not all that hard to understand visually. It's really only a select handful of proofs that necessitated the existence of this document, and even then, part of me still wonders if all of these have some clever visual or intuitive explanation that could cut the text's length in half. Briefly, the molecular approach goes as follows:
\begin{easylist}
  \ListProperties(Style1*=\bfseries,Numbers2=l,Mark1={},Mark2={)},Indent2=1em)
  @ Pick a finite basis of orbitals on which to work
  @@ Usually this will be a selection of the atomic orbitals of each atom, though for computational approaches these "atomic orbitals" are often arbitrary localized wavefunctions which don't actually match the atomic s, p, d, etc orbitals at all
  @@ There are only two important requirements for the basis:
  @@@ It must be invariant under the group - this one's pretty easy to satisfy, as you just need to make sure you include the orbit-wavefunctions of every function you include. E.g. if you include one hydrogen's s orbital in ammonia, you need to include the other two.
  @@ It must be invariant under the Hamiltonian. This one is basically impossible - since we know that in the broader molecule, any space with the right symmetry is going to interact with our chosen subspace. By picking appropriately-energied atomic orbitals, we can greatly reduce the error from restricting ourselves to a finite basis, but there's not a lot we can say conclusively.
  @ Find irreducible subspaces in that basis
  @@ This is the process of labeling orbitals by their symmetry and finding SALCs. The video will cover this process in \textit{significantly} more detail, as it is often unsatisfyingly skipped using character theory.
  @ Construct the combined irreducible representation spaces by grouping the irreducible subspaces by representation
  @ Represent the Hamiltonian numerically on each of the combined spaces
  @ Solve each represented Hamiltonian for the eigenvector basis of that irreducible representation
\end{easylist}

Finally, to connect this to character theory: Character tables store information about the various irreducible representations of a finite group. Specifically, each element in the table represents the \textbf{trace} of the corresponding matrix used to represent $G$. Since we say two representations are equivalent if they are related by a change-of-basis, and the trace doesn't change under change-of-basis, the trace description in a character table is consistent with any representation. Moreover, for reasons that currently bewilder me, the rows of a character table - that is, the sets of traces corresponding to each element of the group (column) from each irreducible representation (row) form an orthogonal vector basis for the space of all representations. Thus, given the representation of $G$ on some reducible subspace, taking a (modified) dot product between the set of traces of that representation and the various rows of the character table can \textbf{tell you what irreducible representations span the space} while knowing almost nothing about the space. 

This becomes incredibly useful with the Projection Operator, which allows you to, given a target irreducible representation, project any vector into that set of all subspaces with that representation. Since character tables gives us a way to know the irreducible representations without doing any actual decomposition, and the projection operator method lets us project into those irreducible representations and thus gives an easy way to find (a) basis, together they skip a massive chunk of the work one would do to decompose these spaces by hand. In the next video and writeup, I'll talk about how and why this works.


Well, that was quite a doozy - but of course, as Huckleberry Finn famously said, "If I'd have had more time, I would've written a shorter letter". I hope this helps \textit{someone} feel better at least; and if not, it'll serve as a nice personal reference guide. Thank you so, so, much for reading, and if you got here without help from the Summer of Math Exposition and 3blue1brown, you should go check them out! \#SoME4 has so many good submissions this year, and most of them aren't even out when I'm writing this! Let's keep math edutainment youtube alive and strong.

















% The benefit of decomposing our space into \textit{invariant} spaces is that the \textbf{representation of our matrix on each individual space} can fully describe the \textbf{action of the matrix on the whole space}. This is can be seen by distributivity across spaces! Say our vector space $V$ can be decomposed into subpsaces $U_1, U_2, ...$, each invariant under our linear operator $A$, such that $V = U_1 \oplus U_2 \oplus ...$ (where $\oplus$ means the direct sum). Since each $U_i$ is invariant under $A$, we can \textbf{represent} $A$ on each $U_i$ via the matrix $B_i$ (though remember we still have to pick a basis in $U_i$ to represent $B_i$ as a matrix on the space. It doesn't matter for this proof). Since $U_1 \oplus U_2 \oplus ... = V$, any vector $v$ in $V$ can be written as $v_1 + v_2 + ...$ for $v_1\in U_1, v_2 \in U_2,$ and so on. This allows us to write $A*v$ as $A*(v_1+v_2+...) = A*v_1 + A*v_2 + ...$. Because each $B_i$ \textit{represents} the action of $A$ on $U_i$, if $v_i$ is in $U_i$, then $A*v_i = B_i*v_i$. Thus: $A*v_1 + A*v_2 + ... = B_1*v_1 + B_2*v_2 + ...$. In other words: If we have a complete set of invariant subspaces and corresponding representations of a linear operator on those subspaces, we can fully represent the operator via its representations on each subspace.
% \footnote{If you're familiar with block-diagonalization, this is essentially equivalent. In fact, in the case of invariant subspaces of the Hamiltonian, it's exactly equivalent. In one of many decisions I make here for the sake of generality, but not necessarily ease of understanding, I have chosen not to go into detail about block-diagonalization here, because it would have been the only time I needed to mention the definition of the quantum version of the dot product. Specifically, if your invariant subspaces are \textit{orthogonal} to each other, meaning that quantum dot product (inner product) between any two vectors from different subspaces is zero, then you can always represent the total matrix in block-diagonal form on those subspaces. Specifically, you choose bases in each of your subspace, and if you represent your action on the whole space in terms of that basis, then the matrix will consist of blocks along the diagonal and zeros everywhere else. Those blocks will themselves be the representations of your linear action on each of the subspaces whose basis vectors correspond to each block. The reason this becomes useful for the Hamiltonian is that it is \textbf{hermitian} - the complex version of symmetric - which means that if a subspace is invariant, then its \textbf{orthogonal complement} - the set of all vectors orthogonal to the invariant subspace is \textit{also} invariant under your action. This is also true for our group actions, because the representation of a finite group is always \textbf{unitary}, which has the same property - the orthogonal complement preserves invariance. In both cases, unitary and hermitian, this is because the matrices always have a complete set of orthogonal eigenvectors.}

% Thus, by finding a complete set of invariant subspaces, we can simplify the process of applying the hamiltonian to a matrix. It might not seem like it at first, but remember - if we're starting with an $D$ dimensional space, a complete representation of the Hamiltonian will have $D^2$ components to calculate. On the other hand, if we split those $D$ dimensions up into, for example, an $N$ dimensional subspace and am $M$ dimensional subspace, the representations on each subspace will only require computing the Hamiltonian on $N^2 + M^2$ components. Since $N + M = D$, this saves us $D^2 - (N^2 + M^2) = (N + M)^2 - N^2 - M^2 = N^2 + 2NM + M^2 - N^2 - M^2 = 2NM$ components. If we can divide into smaller subspaces, we can further reduce this number.

% However, we can do even more. Our ultimate goal is to find the eigenvectors of the Hamiltonian - and this task is inherently linked to invariant subspaces - because 




















% \pagebreak

























% In the field of group theory, the set of all wavefunctions you can get by applying a symmetry to some initial wavefunction is (ironically enough) known as that wavefunction's \textbf{orbit}. Specifically, the orbit of a function $\Psi$ in a group $G$ is defined as $\{g\,*\,\Psi\ |\ g \in G\}$, where $*$ means applying the symmetry action $g$ to $\Psi$. An immediate result is that the maximum size of these orbits is the size of $G$ itself, though of course not all wavefunctions will map to unique orbitals under every symmetry action. 

% Remember that our goal is to find properties of these orbits that will allow us to identify them - and identify which of them are composed of eigenfunctions - in our space of all possible wavefunctions. A property that will help us do that is the fact that orbits are always \textbf{invariant} under the actions of the group. By invariant, I mean that if you apply any symmetry to any element of the set, you will always get another element of the set. After all, if $\chi \in \text{orbit}_G(\Psi)$, then $\chi = g\,*\,\Psi$ for some $g \in G$, meaning $\forall h \in G, h\,*\,(g\,*\,\Psi) = (h\,*\,g)\,*\,\Psi = (hg)\,*\,\Psi$, and since $hg \in G$, $(hg)\,*\,\Psi \in \text{orbit}_G(\Psi)$. 


% In fact, it turns out that orbits are at the core of \textit{every} invariant set - in that if an invariant set is not itself an orbit, it must have a complete set of disjoint (non-overlapping) orbits as subsets (which are themselves also invariant)\footnotemark. Moreover, since the group by definition takes every element of an orbit to every other, an orbit cannot have any invariant subspaces other than itself or the empty set. We therefore say that orbits are \textbf{indecomposable}, or \textbf{irreducible}, invariant subsets - they cannot be broken down into smaller invariant subsets. 
% \footnotetext{This proof is often not given directly, but I'll do it quickly here. \textbf{Theorem:} If a set S is invariant under the action of a group G, then the orbits of G in S partition S. \textbf{Proof:} First, since every element of S is in its own orbit, we know the union of S's orbits must equal S; second, no element of S can be in multiple distinct orbits at once, as any orbit which includes it must have the same orbit as it does. Thus, the orbits of G in S are disjoint and make up S, meaning they partition S.$\square$ This proof is actually a special case of the fact that equivalence classes in general partition a set, and the orbits of G in S form an equivalence class because orbit-membership is transitive, reflexive, and symmetric, the three defining qualities of an equivalence relation. This generalization is how most algebra textbooks prove that orbits partition a set, hence why direct proofs are rare.}

% On the other hand, if the invariant subset is not an orbit, then it can be decomposed into its component orbits, so it \textit{is} \textbf{decomposable}, or \textbf{reducible}\footnotemark. This gives us a recipe for finding the complete set of orbits of some invariant set: since they are the unique invariant subspaces that cannot be decomposed, simply continue decomposing until you can't any more. This can easily be done iteratively by identifying an invariant subset, splitting the larger set into that subset and its complement, and then decomposing those indefinitely.
% \footnotetext{In this document, I'll be treating decomposability and reducibility as synonyms. This is rigorously true for sets, but later we will extend the notion to \textit{linear} group actions on vector spaces, where they are slightly different. Specifically, in the case of infinite dimensions, the definitions are not identical, but we will only be seriously considering finite dimensions here, so the distinction won't come into play.}

% Remember that our goal is to first find all of the sets of equivalent wavefunctions - the orbits - and then later we'll figure out how to narrow it down to the orbits which contain our molecular orbitals. Well, since the action of a symmetry group on a wavefunction will always produce another wavefunction, the set of all wavefunctions is an invariant set. So, if we want to find all the orbits of our molecular wavefunctions, all we need to do is start with the set of all possible wavefunctions and decompose it!

% ...okay, so that's... not exactly practical. Pretty much any way you slice it, our task is going to involve sifting through an infinite number of possible wavefunctions, and while decomposing an infinitely-large set into its infinite number of invariant subsets is \textit{possible} to do analytically, it's a very daunting task. Luckily, this is where the most important property of quantum mechanics comes to our rescue: Linearity. \textit{are}.

% \subsection{Linearity of Molecular Orbitals}

% While we know that our molecular orbitals come in 


% \subsection{Linearity of Group Actions}

% A very useful property of wavefunctions is that they form a vector space - that is, you can add any two wavefunctions and get a wavefunction representing their sum. Moreover, the action of our group on this space is also linear: if you apply a molecular symmetry to the sum of two wavefunctions, the result will be the sum of both wavefunctions operated on individually. This means that we can use the tools of linear algebra to understand our group actions - and linear algebra has several very useful tools to talk about invariance under linear actions.

% The concept which will be the focus of the remainder of this document is that of a \textbf{subspace} of a vector space. A subspace is some subset of a larger vector space that is itself also a vector space - meaning if you add two vectors in that space, you'll always get another vector in that space. For example, in 3 dimensional euclidean space, any plane would be a subspace, as if you add two vectors in a plane, you'll get a third vector in that plane. A line is also a valid subspace of 3d space, as is the set containing just the origin, the zero vector. Finally, 3d space is a subspace of itself, since every set is a subset of itself. Each of these subspaces can be classified by their dimension; a plane is a 2d subspace of the volume, a line is a 1d subspace, and the set containing just 0 - which is often called the trivial subspace, as it's a subspace of every vector space - is 0-dimensional. 






% \subsection{Quantum Mechanics, the Hamiltonian, and Eigenfunctions}
% As mentioned above, the molecular orbtials of a molecule are those which "don't change over time". Thus, to understand what our molecular orbitals are, we need to know how orbitals change over time. Luckily, there's a handy-dandy equation which lays this out for us: the \textbf{Schr{\"o}dinger Equation.}
% \begin{figure}[H]
%     \centering
%     \begin{equation}
%         \frac{\partial{\Psi}}{\partial{t}} = -\frac{i}{\hbar}\ham\Psi
%     \end{equation}
%     \caption{The Schr{\"o}dinger Equation}
%     \label{fig:schrodinger}
% \end{figure}

% Since we're looking for stable molecular orbitals, we use the time-dependent solution: $\ham\Psi = E\Psi$. There's actually a very visually satisfying explanation of this solution, but it doesn't translate well in text, so I'll be skipping it here\footnotemark. However, it's critical to understand what exactly this solution means, and it's all about $\ham$: The Hamiltonian.
% \footnotetext{Text version if you want it anyway: we need a wavefunction whose \textit{magnitude} doesn't change over time, though its phase can potentially change over time (since a wavefunction is complex-valued). Thus, the change in a wavefunction's value over time must either be zero, or a value which will only change its phase. Recall that in complex-space, multiplication by i is like rotation by 90*. Thus, whatever the value of $\ham\Psi$ is will be rotated by 90* angle and then added to $\Psi$. Since the change is instantaneous, so long as $\frac{\partial{\Psi}}{\partial{\Psi}}$ is perpendicular to $\Psi$, $\Psi$ will always maintain a constant magnitude. Since $i\ham\Psi$ is itself perpendicular to $\ham\Psi$, $\ham\Psi$ must be perpendicular to something perpendicular to $\Psi$, so it must be parallel to $\Psi$ in (2d) complex space - in other words, it must be some \textit{real} scalar multiple of $\Psi$. The fact that this scalar value $E$ also represents the energy of that wavefunction is a fascinating topic best left to time-symmetry, philosophizers, and string theorists.}

% The Hamiltonian is what is known as a linear operator. You can think of it as a more general version of a square matrix: When $\ham$ is multiplied by a wavefunction, it returns another wavefunction ("operator"), and it does so "linearly", meaning if $\Psi = a\Psi_1 + b\Psi_2$, then $\ham\Psi = \ham(a\Psi_1 + b\Psi_2) = a\ham\Psi_1 + b\ham\Psi_2$ for scalar $a$ and $b$.

% The Hamiltonian is critical to both the definition and the solution to the Schr\"odinger equation: because $-\frac{i}{\hbar}$ is just a constant, $\ham\Psi$ almost entirely defines the change of any system over time. Its therefore necessarily encapsulates all physical information about a system, including the potential field wihch causes orbitals to evolve and the energies of said orbitals. 






% \pagebreak


% NOTES TO SELF:
% \begin{enumerate}
%     \item UPDATE THE "keep reading for phase offset" FOOTNOTE TO REFER TO THE ACTUAL HAMILTONIAN SECTION ONCE I WRITE IT
%     \item Add an outline to the beginning that describes the structure of the video, 3b1b-style
% \end{enumerate}

\end{document}
